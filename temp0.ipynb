{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData():\n",
    "    image_list = []\n",
    "    myclass = []\n",
    "    for filename in glob.glob('dataset-resized/glass/*.jpg'): #assuming gif\n",
    "        im=np.array(Image.open(filename))\n",
    "        myclass.append(0)\n",
    "        image_list.append(im)\n",
    "    for filename in glob.glob('dataset-resized/cardboard/*.jpg'): #assuming gif\n",
    "        im=np.array(Image.open(filename))\n",
    "        image_list.append(im)\n",
    "        myclass.append(1)\n",
    "    for filename in glob.glob('dataset-resized/metal/*.jpg'): #assuming gif\n",
    "        im=np.array(Image.open(filename))\n",
    "        image_list.append(im)\n",
    "        myclass.append(2)\n",
    "    for filename in glob.glob('dataset-resized/paper/*.jpg'): #assuming gif\n",
    "        im=np.array(Image.open(filename))\n",
    "        image_list.append(im)\n",
    "        myclass.append(3)\n",
    "    \n",
    "    myclass = keras.utils.to_categorical(np.asarray(myclass), num_classes=4)\n",
    "    return image_list, myclass\n",
    "\n",
    "\n",
    "[x,y] = readData()\n",
    "x = np.asarray(x)\n",
    "# pca\n",
    "cv = KFold(n_splits = 10, random_state = 43)\n",
    "for index_train, index_test in cv.split(x):\n",
    "    trainsize = index_train.size\n",
    "    testsize = index_test.size\n",
    "    train_dataset = x[index_train]\n",
    "    test_dataset = x[index_test]\n",
    "    train_labels = y[index_train] \n",
    "    test_labels = y[index_test]\n",
    "    classifier(train_dataset, test_dataset, train_labels, test_labels, trainsize, testsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(train_dataset, test_dataset, train_labels, test_labels, trainsize, testsize):\n",
    "    batch_size = 16\n",
    "    patch_size = 5\n",
    "    image_size1 = 384 \n",
    "    image_size2 = 512\n",
    "    depth = 16\n",
    "    num_hidden1 = 256\n",
    "    num_hidden2 = 64\n",
    "    num_channels = 3\n",
    "    num_labels = 4\n",
    "    \n",
    "    train_dataset = train_dataset.reshape(\n",
    "            (trainsize, image_size1, image_size2, num_channels)).astype(np.float32)\n",
    "    test_dataset = test_dataset.reshape(\n",
    "            (testsize, image_size1, image_size2, num_channels)).astype(np.float32)\n",
    "     \n",
    "    def accuracy(predictions, labels):\n",
    "        return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0]) \n",
    "      \n",
    "    graph = tf.Graph()\n",
    "\n",
    "    with graph.as_default():\n",
    "        # Define the training dataset and lables\n",
    "        tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size1, image_size2, num_channels))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "        # Validation/test dataset\n",
    "        tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "        # CNN layer 1 with filter (num_channels, depth) (3, 16)\n",
    "        cnn1_W = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "        cnn1_b = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "        # CNN layer 2 with filter (depth, depth) (16, 16)\n",
    "        cnn2_W = tf.Variable(tf.truncated_normal(\n",
    "            [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "        cnn2_b = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "        # Compute the output size of the CNN2 as a 1D array.\n",
    "        size = image_size1 // 4 * image_size2 // 4 * depth\n",
    "\n",
    "        # FC1 (size, num_hidden1) (size, 256)\n",
    "        fc1_W = tf.Variable(tf.truncated_normal(\n",
    "            [size, num_hidden1], stddev=np.sqrt(2.0 / size)))\n",
    "        fc1_b = tf.Variable(tf.constant(1.0, shape=[num_hidden1]))\n",
    "\n",
    "        # FC2 (num_hidden1, num_hidden2) (size, 64)\n",
    "        fc2_W = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden1, num_hidden2], stddev=np.sqrt(2.0 / (num_hidden1))))\n",
    "        fc2_b = tf.Variable(tf.constant(1.0, shape=[num_hidden2]))\n",
    "\n",
    "        # Classifier (num_hidden2, num_labels) (64, 10)\n",
    "        classifier_W = tf.Variable(tf.truncated_normal(\n",
    "            [num_hidden2, num_labels], stddev=np.sqrt(2.0 / (num_hidden2))))\n",
    "        classifier_b = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "        # Model.\n",
    "        def model(data):\n",
    "        # First convolution layer with stride = 1 and pad the edge to make the output size the same.\n",
    "        # Apply ReLU and a maximum 2x2 pool\n",
    "            conv1 = tf.nn.conv2d(data, cnn1_W, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden1 = tf.nn.relu(conv1 + cnn1_b)\n",
    "            pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            # Second convolution layer\n",
    "            conv2 = tf.nn.conv2d(pool1, cnn2_W, [1, 1, 1, 1], padding='SAME')\n",
    "            hidden2 = tf.nn.relu(conv2 + cnn2_b)\n",
    "            pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "            # Flattern the convolution output\n",
    "            shape = pool2.get_shape().as_list()\n",
    "            reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "            # 2 FC hidden layers\n",
    "            fc1 = tf.nn.relu(tf.matmul(reshape, fc1_W) + fc1_b)\n",
    "            fc2 = tf.nn.relu(tf.matmul(fc1, fc2_W) + fc2_b)\n",
    "\n",
    "            # Return the result of the classifier\n",
    "            return tf.matmul(fc2, classifier_W) + classifier_b\n",
    "\n",
    "        # Training computation.\n",
    "        logits = model(tf_train_dataset)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "    num_steps = 20001\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
