{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.7\n",
    "NUM_CLASSES = 3\n",
    "RESIZE_IMAGE_SHAPE = (32,32)\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "PATCH_SIZE = 5\n",
    "DEPTH = 16\n",
    "NUM_HIDDEN1 = 256\n",
    "NUM_HIDDEN2 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_images(image_list):\n",
    "    for image in image_list:\n",
    "        img = Image.fromarray(image, 'RGB')\n",
    "        img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read & Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_split_data():\n",
    "    image_list = []\n",
    "    myclass = []\n",
    "    for filename in glob.glob('Dataset/Compost/*.jpg'):\n",
    "        img = Image.open(filename)\n",
    "        img = img.resize(RESIZE_IMAGE_SHAPE, Image.ANTIALIAS)\n",
    "        img = np.array(img)\n",
    "        image_list.append(img)\n",
    "        myclass.append(0)\n",
    "    for filename in glob.glob('Dataset/Landfill/*.jpg'):\n",
    "        img = Image.open(filename)\n",
    "        img = img.resize(RESIZE_IMAGE_SHAPE, Image.ANTIALIAS)\n",
    "        img = np.array(img)\n",
    "        image_list.append(img)\n",
    "        myclass.append(1)\n",
    "    for filename in glob.glob('Dataset/Recyclable/*.jpg'):\n",
    "        img = Image.open(filename)\n",
    "        img = img.resize(RESIZE_IMAGE_SHAPE, Image.ANTIALIAS)\n",
    "        img = np.array(img)\n",
    "        image_list.append(img)\n",
    "        myclass.append(2)\n",
    "    \n",
    "    X = np.asarray(image_list, dtype='float32')\n",
    "    y = keras.utils.to_categorical(np.asarray(myclass), num_classes=NUM_CLASSES)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=TRAIN_SIZE)\n",
    "    \n",
    "    return len(X_train), len(X_test), X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(data, cnn1_W, cnn1_b, cnn2_W, cnn2_b, fc1_W, fc1_b, fc2_W, fc2_b, classifier_W, classifier_b):\n",
    "    # First convolution layer with stride = 1 and pad the edge to make the output size the same.\n",
    "    # Apply ReLU and a maximum 2x2 pool\n",
    "    conv1 = tf.nn.conv2d(data, cnn1_W, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden1 = tf.nn.relu(conv1 + cnn1_b)\n",
    "    pool1 = tf.nn.max_pool(hidden1, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Second convolution layer\n",
    "    conv2 = tf.nn.conv2d(pool1, cnn2_W, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden2 = tf.nn.relu(conv2 + cnn2_b)\n",
    "    pool2 = tf.nn.max_pool(hidden2, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "    # Flattern the convolution output\n",
    "    shape = pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "    # 2 FC hidden layers\n",
    "    fc1 = tf.nn.relu(tf.matmul(reshape, fc1_W) + fc1_b)\n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, fc2_W) + fc2_b)\n",
    "\n",
    "    # Return the result of the classifier\n",
    "    return tf.matmul(fc2, classifier_W) + classifier_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(train_size, test_size, X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    train_dataset = X_train.reshape((train_size, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype(np.float32)\n",
    "    test_dataset = X_test.reshape((test_size, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)).astype(np.float32)\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    with graph.as_default():\n",
    "        \n",
    "        # Define the training dataset and lables\n",
    "        tf_train_dataset = tf.placeholder(tf.float32, shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "        tf_train_labels = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_CLASSES))\n",
    "\n",
    "        # Validation/test dataset\n",
    "        tf_test_dataset = tf.constant(X_test)\n",
    "    \n",
    "        # CNN layer 1 with filter (num_channels, depth) (3, 16)\n",
    "        cnn1_W = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, NUM_CHANNELS, DEPTH], stddev=0.1))\n",
    "        cnn1_b = tf.Variable(tf.zeros([DEPTH]))\n",
    "\n",
    "        # CNN layer 2 with filter (depth, depth) (16, 16)\n",
    "        cnn2_W = tf.Variable(tf.truncated_normal([PATCH_SIZE, PATCH_SIZE, DEPTH, DEPTH], stddev=0.1))\n",
    "        cnn2_b = tf.Variable(tf.constant(1.0, shape=[DEPTH]))\n",
    "\n",
    "        # Compute the output size of the CNN2 as a 1D array.\n",
    "        # CHECK!!!!\n",
    "        size = IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * DEPTH\n",
    "\n",
    "        # FC1 (size, num_hidden1) (size, 256)\n",
    "        fc1_W = tf.Variable(tf.truncated_normal([size, NUM_HIDDEN1], stddev=np.sqrt(2.0 / size)))\n",
    "        fc1_b = tf.Variable(tf.constant(1.0, shape=[NUM_HIDDEN1]))\n",
    "\n",
    "        # FC2 (num_hidden1, num_hidden2) (size, 64)\n",
    "        fc2_W = tf.Variable(tf.truncated_normal([NUM_HIDDEN1, NUM_HIDDEN2], stddev=np.sqrt(2.0/(NUM_HIDDEN1))))\n",
    "        fc2_b = tf.Variable(tf.constant(1.0, shape=[NUM_HIDDEN2]))\n",
    "\n",
    "        # Classifier (num_hidden2, num_labels) (64, 10)\n",
    "        classifier_W = tf.Variable(tf.truncated_normal([NUM_HIDDEN2, NUM_CLASSES], stddev=np.sqrt(2.0 / (NUM_HIDDEN2))))\n",
    "        classifier_b = tf.Variable(tf.constant(1.0, shape=[NUM_CLASSES]))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Training computation.\n",
    "        logits = cnn_model(tf_train_dataset, cnn1_W, cnn1_b, cnn2_W, cnn2_b, \n",
    "                           fc1_W, fc1_b, fc2_W, fc2_b, classifier_W, classifier_b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "        # Optimizer.\n",
    "        optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n",
    "\n",
    "        # Predictions for the training, validation, and test data.\n",
    "        train_prediction = tf.nn.softmax(logits)\n",
    "        test_prediction = tf.nn.softmax(cnn_model(tf_test_dataset, cnn1_W, cnn1_b, cnn2_W, cnn2_b, \n",
    "                                                  fc1_W, fc1_b, fc2_W, fc2_b, classifier_W, classifier_b))\n",
    "\n",
    "        \n",
    "        \n",
    "    num_steps = 20001\n",
    "    \n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized')\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * BATCH_SIZE) % (y_train.shape[0] - BATCH_SIZE)\n",
    "            batch_data = train_dataset[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "            batch_labels = y_train[offset:(offset + BATCH_SIZE), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "            if (step % 500 == 0):\n",
    "                print('Minibatch loss at step %d: %f' % (step, l))\n",
    "                print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "                print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHECK: Figure out what this is doing!!\n",
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "            / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranavhgupta/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 153.656219\n",
      "Minibatch accuracy: 31.2%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 500: 0.000011\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 11.1%\n",
      "Minibatch loss at step 1000: 0.000003\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 1500: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 33.3%\n",
      "Minibatch loss at step 2000: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 3500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 4000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 33.3%\n",
      "Minibatch loss at step 4500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 33.3%\n",
      "Minibatch loss at step 5000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 5500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 6000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 6500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 7000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 7500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 8000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 8500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 9000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 9500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 44.4%\n",
      "Minibatch loss at step 10000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 44.4%\n",
      "Minibatch loss at step 10500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 55.6%\n",
      "Minibatch loss at step 11000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 33.3%\n",
      "Minibatch loss at step 11500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 11.1%\n",
      "Minibatch loss at step 12000: 0.000106\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 44.4%\n",
      "Minibatch loss at step 12500: 0.000004\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 33.3%\n",
      "Minibatch loss at step 13000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 33.3%\n",
      "Minibatch loss at step 13500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 33.3%\n",
      "Minibatch loss at step 14000: 0.000720\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 14500: 0.000019\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 15000: 0.000011\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 15500: 0.000007\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 11.1%\n",
      "Minibatch loss at step 16000: 0.000003\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 16500: 0.000002\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 17000: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 17500: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 18000: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 18500: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 19000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 19500: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 22.2%\n",
      "Minibatch loss at step 20000: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Test accuracy: 11.1%\n"
     ]
    }
   ],
   "source": [
    "train_size, test_size, X_train, X_test, y_train, y_test = read_split_data()\n",
    "\n",
    "# view_images(X_test)\n",
    "\n",
    "train_cnn(train_size, test_size, X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
